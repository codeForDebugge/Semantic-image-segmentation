{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,input_size,layers,output_size):\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        model = {} #Dictionary\n",
    "        \n",
    "        #First Layer\n",
    "        model['W1'] = np.random.randn(input_size,layers[0])\n",
    "        model['b1'] = np.zeros((1,layers[0]))\n",
    "        \n",
    "        #Second Layer\n",
    "        model['W2'] = np.random.randn(layers[0],layers[1])\n",
    "        model['b2'] = np.zeros((1,layers[1]))\n",
    "        \n",
    "        #Third/Output Layer\n",
    "        model['W3'] = np.random.randn(layers[1],output_size)\n",
    "        model['b3'] = np.zeros((1,output_size))\n",
    "        \n",
    "        self.model = model\n",
    "        self.activation_outputs = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        W1,W2,W3 = self.model['W1'],self.model['W2'],self.model['W3']\n",
    "        b1, b2, b3 = self.model['b1'],self.model['b2'],self.model['b3']\n",
    "        \n",
    "        z1 = np.dot(x,W1) + b1\n",
    "        a1 = np.tanh(z1) \n",
    "        \n",
    "        z2 = np.dot(a1,W2) + b2\n",
    "        a2 = np.tanh(z2)\n",
    "        \n",
    "        z3 = np.dot(a2,W3) + b3\n",
    "        y_ = softmax(z3)\n",
    "        \n",
    "        self.activation_outputs = (a1,a2,y_)\n",
    "        return y_\n",
    "        \n",
    "    def backward(self,x,y,learning_rate=0.001):\n",
    "        W1,W2,W3 = self.model['W1'],self.model['W2'],self.model['W3']\n",
    "        b1, b2, b3 = self.model['b1'],self.model['b2'],self.model['b3']\n",
    "        m = x.shape[0]\n",
    "        \n",
    "        a1,a2,y_ = self.activation_outputs\n",
    "        \n",
    "        delta3 = y_ - y\n",
    "        dw3 = np.dot(a2.T,delta3)\n",
    "        db3 = np.sum(delta3,axis=0)\n",
    "        \n",
    "        delta2 = (1-np.square(a2))*np.dot(delta3,W3.T)\n",
    "        dw2 = np.dot(a1.T,delta2)\n",
    "        db2 = np.sum(delta2,axis=0)\n",
    "        \n",
    "        delta1 = (1-np.square(a1))*np.dot(delta2,W2.T)\n",
    "        dw1 = np.dot(X.T,delta1)\n",
    "        db1 = np.sum(delta1,axis=0)\n",
    "        \n",
    "        \n",
    "        #Update the Model Parameters using Gradient Descent\n",
    "        self.model[\"W1\"]  -= learning_rate*dw1\n",
    "        self.model['b1']  -= learning_rate*db1\n",
    "        \n",
    "        self.model[\"W2\"]  -= learning_rate*dw2\n",
    "        self.model['b2']  -= learning_rate*db2\n",
    "        \n",
    "        self.model[\"W3\"]  -= learning_rate*dw3\n",
    "        self.model['b3']  -= learning_rate*db3\n",
    "        \n",
    "        # :)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        y_out = self.forward(x)\n",
    "        return np.argmax(y_out,axis=1)\n",
    "    \n",
    "    def summary(self):\n",
    "        W1,W2,W3 = self.model['W1'],self.model['W2'],self.model['W3']\n",
    "        a1,a2,y_ = self.activation_outputs\n",
    "        \n",
    "        print(\"W1 \",W1.shape)\n",
    "        print(\"A1 \",a1.shape)\n",
    "\n",
    "def softmax(a):\n",
    "    e_pa = np.exp(a) #Vector\n",
    "    ans = e_pa/np.sum(e_pa,axis=1,keepdims=True)\n",
    "    return ans        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
